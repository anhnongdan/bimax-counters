#!/usr/bin/python
#import multiprocessing
import os
import time

#the_queue = multiprocessing.Queue()
#mainQueue = multiprocessing.Queue()

from threading import Thread
from Queue import Queue
import sys, time
import logging
logging.basicConfig(filename='/tmp/scribe_pipe.log',level=logging.DEBUG)

from nginx_log_parser import NginxLogParser
from dateutil.parser import parse

from datetime import datetime, timedelta
from pytz import timezone, utc
from pytz.tzinfo import StaticTzInfo

class OffsetTime(StaticTzInfo):
    def __init__(self, offset):
        """A dumb timezone based on offset such as +0530, -0600, etc.
        """
        hours = int(offset[:3])
        minutes = int(offset[0] + offset[3:])
        self._utcoffset = timedelta(hours=hours, minutes=minutes)

def load_datetime(value, format):
    if format.endswith('%z'):
        format = format[:-2]
        offset = value[-5:]
        value = value[:-5]
        return OffsetTime(offset).localize(datetime.strptime(value, format))

    return datetime.strptime(value, format)

def dump_datetime(value, format):
    return value.strftime(format)
#vnpt-hni-14 - - [meta sequenceId="49552677"] nginx:
#parser = NginxLogParser('$request_time $remote_addr $sent_http_x_cache [$time_local] ' + \
parser = NginxLogParser('$remote_host - - [$seq_id] nginx: $request_time $remote_addr $sent_http_x_cache [$time_local] ' + \
			'"$request" $http_host $status $body_bytes_sent ' + \
			'"$http_referer" "$http_user_agent" "$http_range"')
import redis

rd = redis.Redis(unix_socket_path='/tmp/redis.sock', db=0)

NTIME = 3
#GLOBALLOCK = multiprocessing.Lock()

num_worker_process = 4
num_worker_main = 4
# t_timeout = 10
tcount = 50000
#tqueue = []

def check_time1(atime):
    cur = datetime.now()
    min5 = cur.minute - (cur.minute % 5);
    cur5 = cur.replace(minute=min5)
    nn = cur5.strftime("5:%Y%m%d%H%M%z+0700")

    if atime != nn:
        return False
    return True

def check_time(tqueue1):
    cur = datetime.now()
    min5 = cur.minute - (cur.minute % 5);
    cur5 = cur.replace(minute=min5)
    nn = cur5.strftime("5:%Y%m%d%H%M%z+0700")
    tt = tqueue1[0]
    if tt['name'] != nn:
        return False
    return True

def flush_queue():
    if len(tqueue) > 0 and check_time(tqueue):
        #GLOBALLOCK.acquire()
        the_queue.put(tqueue[:])
        tqueue[:] = []
        #GLOBALLOCK.release()
    else:
        #logging.info("flush: onrotate")
        tqueue[:] = []

import uuid

def worker(myqueue):
    id=uuid.uuid4()

    cache_val = {}
    logging.info("process start:%s" % id)
    rd = redis.Redis(unix_socket_path='/tmp/redis.sock', db=0)
    tqueue1 = []
    oldut = 0
    while True:
        try:
            tqueue1 = myqueue.get()
            n = len(tqueue1)
            if n == 0:
                continue
            ss = len(tqueue1)
            cur = datetime.now()
            ut = int(time.mktime(cur.timetuple()))

            if not check_time(tqueue1):
                #logging.info("flush: onprocess")
                continue

            send = False
            if ut - oldut > NTIME:
                oldut = ut
                send = True

            for tval in tqueue1:
                kk = "%s###%s" %(tval['type'], tval['name'])
                if kk in cache_val:
                    cache_val[kk] = cache_val[kk] + float(tval['val'])
                else:
                    cache_val[kk] = float(tval['val'])
            if send:
               # logging.info("send:%s:%d" % (id, len(cache_val)))
                pp = rd.pipeline()
                for kk in cache_val:
                    vv = float(cache_val[kk])
                    type,name = kk.split("###")
                    pp.hincrbyfloat(type, name, vv)
                pp.execute()
                cache_val = {}
        except Exception, err:
            logging.info(err)
            continue


def do_flush(cache_val):
    pp = rd.pipeline()
    for kk in cache_val:
         vv = float(cache_val[kk])
         type,name = kk.split("###")
         pp.hincrbyfloat(type, name, vv)
    pp.execute()

def do_count(tqueue):
    cache_val = {}
    for tval in tqueue:
        kk = "%s###%s" %(tval['type'], tval['name'])
        if kk in cache_val:
            cache_val[kk] = cache_val[kk] + float(tval['val'])
        else:
            cache_val[kk] = float(tval['val'])
    do_flush(cache_val)


def workerMain(id, mainQueue):
    #id=uuid.uuid4()
    logging.info("worker:%s" % id)
    cur = datetime.now()
    utime = int(time.mktime(cur.timetuple()))
    tqueue = []
    while True:
        try:
            if mainQueue.empty():
                continue
            message = mainQueue.get_nowait()
            if not message:
                continue
            pp = parser.parse_line(message)
            if not pp:
                continue
            #logging.info(pp)
            rhost = pp["remote_host"]
            if pp['http_host']:
                cat = pp['http_host']
            else:
                cat = category

            if pp['time_local'] is None or pp['status'] is None or pp['body_bytes_sent'] is None or pp['request_time'] is None:
                continue

            cur = load_datetime(pp['time_local'], '%d/%b/%Y:%H:%M:%S %z')
            localtime = dump_datetime(cur, "%Y%m%d%H%M%z")
            min5 = cur.minute - (cur.minute % 5);
            cur5 = cur.replace(minute=min5)
            localtime5 = dump_datetime(cur5, "5:%Y%m%d%H%M%z")
            if not check_time1(localtime5):
#                logging.info("flush: onreal:%s" % localtime5)
                continue

            tqueue.append({
                "type": "rhost_request_count_%s|%s" % (str(pp["status"]), str(rhost)),
                "name":localtime5,
                "val":1})
            tqueue.append({
                "type": "request_count_%s|%s" % (str(pp["status"]), str(cat)),
                "name":localtime5,
                "val":1})
            if pp['status'] == '200' or  pp['status'] == '206':
                tqueue.append({"type": "rhost_body_bytes_sent" + "|"  + rhost, "name": localtime5, "val":pp["body_bytes_sent"]})
                tqueue.append({"type": "body_bytes_sent" + "|"  + cat , "name": localtime5, "val":pp["body_bytes_sent"]})
                tqueue.append({"type": "rhost_request_time" + "|"  + rhost, "name":localtime5, "val":pp["request_time"]})
                tqueue.append({"type": "request_time" + "|"  + cat , "name":localtime5, "val":pp["request_time"]})


                if float(pp["request_time"]) > 0.0000001:
                    speed_request = float(pp["body_bytes_sent"]) / float(pp["request_time"])
                    tqueue.append({"type": "speed_request" + "|"  + cat , "name": localtime5, "val": speed_request})
                    tqueue.append({"type": "rhost_speed_request" + "|"  + rhost, "name": localtime5, "val": speed_request})
                    tqueue.append({
                        "type": "rhost_request_count_2xx|%s" % str(rhost),
                        "name":localtime5,
                        "val":1})
                    tqueue.append({
                        "type": "request_count_2xx|%s" % str(cat),
                        "name":localtime5,
                        "val":1})

            cur1 = datetime.now()
            utime1 = int(time.mktime(cur.timetuple()))
            if (utime1 - utime) > NTIME or len(tqueue) >= tcount:
                logging.info("flush:%s" % len(tqueue))
                utime = utime1
                do_count(tqueue)
                tqueue[:] = []
                cache_val = {}
            #    logging.info("tqueue:%s" % len(tqueue))
            #    logging.info("cache_val:%s" % len(cache_val))
        except Exception, err:
            logging.info(err)
            continue


q_msg = Queue()
NPIPE_INIT=1
POOL = []
def create_thread(q_msg):
    i = len(POOL) + 1
    logging.info("create thread:%s" %i)
    t = Thread(target=workerMain, name='q_msg_%d' % i,args=(i, q_msg))
    POOL.append(t)
    t.daemon = True
    t.start()

for i in range(NPIPE_INIT):
    create_thread(q_msg)



#the_pool = multiprocessing.Pool(num_worker_process, worker,(the_queue,))
#mainPool = multiprocessing.Pool(num_worker_main, workerMain,(mainQueue,))

cur0 = datetime.now()
utime0 = int(time.mktime(cur0.timetuple()))



while True:
    try:
        message = sys.stdin.readline()
        if not message:
            continue
        message = message.replace('""', '"-"')
	#logging.info(message)
        q_msg.put_nowait(message)
        #mainQueue.put(message)

        cur1 = datetime.now()
        utime1 = int(time.mktime(cur1.timetuple()))
        if utime1 - utime0 > 10:
            qs = q_msg.qsize()
            if qs > 10000:
                create_thread(q_msg)
            logging.info(q_msg.qsize())
            cur0 = datetime.now()
            utime0 = int(time.mktime(cur0.timetuple()))

    except Exception, err:
        logging.info(err)
        pass
